{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pandas import Categorical, get_dummies\n",
    "from sklearn.base import TransformerMixin, BaseEstimator\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from sklearn.preprocessing import Imputer, StandardScaler\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.externals import joblib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['housing.csv', 'housing.tgz', 'README.md']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>longitude</th>\n",
       "      <th>latitude</th>\n",
       "      <th>housing_median_age</th>\n",
       "      <th>total_rooms</th>\n",
       "      <th>total_bedrooms</th>\n",
       "      <th>population</th>\n",
       "      <th>households</th>\n",
       "      <th>median_income</th>\n",
       "      <th>median_house_value</th>\n",
       "      <th>ocean_proximity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-122.23</td>\n",
       "      <td>37.88</td>\n",
       "      <td>41.0</td>\n",
       "      <td>880.0</td>\n",
       "      <td>129.0</td>\n",
       "      <td>322.0</td>\n",
       "      <td>126.0</td>\n",
       "      <td>8.3252</td>\n",
       "      <td>452600.0</td>\n",
       "      <td>NEAR BAY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-122.22</td>\n",
       "      <td>37.86</td>\n",
       "      <td>21.0</td>\n",
       "      <td>7099.0</td>\n",
       "      <td>1106.0</td>\n",
       "      <td>2401.0</td>\n",
       "      <td>1138.0</td>\n",
       "      <td>8.3014</td>\n",
       "      <td>358500.0</td>\n",
       "      <td>NEAR BAY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-122.24</td>\n",
       "      <td>37.85</td>\n",
       "      <td>52.0</td>\n",
       "      <td>1467.0</td>\n",
       "      <td>190.0</td>\n",
       "      <td>496.0</td>\n",
       "      <td>177.0</td>\n",
       "      <td>7.2574</td>\n",
       "      <td>352100.0</td>\n",
       "      <td>NEAR BAY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-122.25</td>\n",
       "      <td>37.85</td>\n",
       "      <td>52.0</td>\n",
       "      <td>1274.0</td>\n",
       "      <td>235.0</td>\n",
       "      <td>558.0</td>\n",
       "      <td>219.0</td>\n",
       "      <td>5.6431</td>\n",
       "      <td>341300.0</td>\n",
       "      <td>NEAR BAY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-122.25</td>\n",
       "      <td>37.85</td>\n",
       "      <td>52.0</td>\n",
       "      <td>1627.0</td>\n",
       "      <td>280.0</td>\n",
       "      <td>565.0</td>\n",
       "      <td>259.0</td>\n",
       "      <td>3.8462</td>\n",
       "      <td>342200.0</td>\n",
       "      <td>NEAR BAY</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   longitude  latitude  housing_median_age  total_rooms  total_bedrooms  \\\n",
       "0    -122.23     37.88                41.0        880.0           129.0   \n",
       "1    -122.22     37.86                21.0       7099.0          1106.0   \n",
       "2    -122.24     37.85                52.0       1467.0           190.0   \n",
       "3    -122.25     37.85                52.0       1274.0           235.0   \n",
       "4    -122.25     37.85                52.0       1627.0           280.0   \n",
       "\n",
       "   population  households  median_income  median_house_value ocean_proximity  \n",
       "0       322.0       126.0         8.3252            452600.0        NEAR BAY  \n",
       "1      2401.0      1138.0         8.3014            358500.0        NEAR BAY  \n",
       "2       496.0       177.0         7.2574            352100.0        NEAR BAY  \n",
       "3       558.0       219.0         5.6431            341300.0        NEAR BAY  \n",
       "4       565.0       259.0         3.8462            342200.0        NEAR BAY  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(os.listdir('./datasets/housing'))\n",
    "\n",
    "housing = pd.read_csv('./datasets/housing/housing.csv')\n",
    "housing.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(16512, 9)\n"
     ]
    }
   ],
   "source": [
    "housing[\"income_cat\"] = np.ceil(housing[\"median_income\"] / 1.5)\n",
    "#print(housing.income_cat)\n",
    "# note this code. It's so nice that series also have where!!!\n",
    "housing[\"income_cat\"].where(housing[\"income_cat\"] < 5, 5.0, inplace=True)\n",
    "# housing.income_cat.hist(bins = np.arange(6)-0.5);\n",
    "# plt.hist(housing.income_cat)\n",
    "\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "\n",
    "mysplit = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=42)\n",
    "# since n_splits=1 we have only one list for train_index, one list for test_index\n",
    "for train_index, test_index in mysplit.split(housing, housing['income_cat']):\n",
    "    train_data = housing.loc[train_index]\n",
    "    test_data = housing.loc[test_index]\n",
    "\n",
    "xtrain = train_data.drop(columns=['median_house_value', 'income_cat'])\n",
    "ytrain = train_data['median_house_value']\n",
    "\n",
    "xtest = test_data.drop(columns=['median_house_value', \"income_cat\"])\n",
    "ytest = test_data['median_house_value']\n",
    "print(xtrain.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Handling numeric features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For imputing the missing values for numeric features, use only knowledge from train set. This mean \"fit\" using the train set and then \"transform\" for both train and test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Selector to just select the data\n",
    "class DataFrameSelector(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, attribute_names):\n",
    "        self.attribute_names = attribute_names\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    def transform(self, X):\n",
    "        return X[self.attribute_names].values\n",
    "\n",
    "# column numbers of some attributes\n",
    "rooms_ix, bedrooms_ix, population_ix, household_ix = 3, 4, 5, 6    \n",
    "class CombinedAttributesAdder(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, add_bedrooms_per_room = True): # no *args or **kargs\n",
    "        self.add_bedrooms_per_room = add_bedrooms_per_room\n",
    "        \n",
    "    def fit(self, X, y=None):\n",
    "        return self # nothing else to do\n",
    "    \n",
    "    def transform(self, X, y=None):\n",
    "        # 2 features are always added\n",
    "        rooms_per_household = X[:, rooms_ix] / X[:, household_ix]\n",
    "        population_per_household = X[:, population_ix] / X[:, household_ix]\n",
    "        # one feature can be added or not based on the boolean parameter\n",
    "        if self.add_bedrooms_per_room:\n",
    "            bedrooms_per_room = X[:, bedrooms_ix] / X[:, rooms_ix]\n",
    "            # np.c_ Translates slice objects to concatenation along the second axis.\n",
    "            return np.c_[X, rooms_per_household, population_per_household, bedrooms_per_room]\n",
    "        \n",
    "        else:\n",
    "            #print(X.shape)\n",
    "            #print(rooms_per_household.shape)\n",
    "            #print(np.c_[X, rooms_per_household, population_per_household].shape)\n",
    "            return np.c_[X, rooms_per_household, population_per_household]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(16512, 11)\n",
      "(4128, 11)\n"
     ]
    }
   ],
   "source": [
    "num_att = list(xtrain.drop(columns=[\"ocean_proximity\"]))\n",
    "\n",
    "num_pipeline = Pipeline([(\"selector\", DataFrameSelector(num_att)), \n",
    "                         (\"imputer\", Imputer(strategy=\"median\")),\n",
    "                        (\"add_attributes\", CombinedAttributesAdder()),\n",
    "                        (\"std_scaler\", StandardScaler())])\n",
    "\n",
    "num_pipeline.fit(xtrain)\n",
    "xtrain_num = num_pipeline.transform(xtrain)\n",
    "xtest_num = num_pipeline.transform(xtest)\n",
    "print(xtrain_num.shape)\n",
    "print(xtest_num.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Handling categorical features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to avoid the situation that some categories exist in the test set, but not in the train set, I would prefer not to use pipeline here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(16512, 5)\n",
      "(4128, 5)\n",
      "['ocean_proximity_NEAR BAY', 'ocean_proximity_<1H OCEAN', 'ocean_proximity_INLAND', 'ocean_proximity_NEAR OCEAN', 'ocean_proximity_ISLAND']\n"
     ]
    }
   ],
   "source": [
    "# get all possible categories\n",
    "cat_cols = [\"ocean_proximity\"]\n",
    "for col in cat_cols:\n",
    "    possible_categories = housing[col].unique().tolist()\n",
    "    xtrain[col] = pd.Categorical(xtrain[col], categories=possible_categories)\n",
    "    xtest[col] = pd.Categorical(xtest[col], categories=possible_categories)\n",
    "\n",
    "xtrain_cat = pd.get_dummies(xtrain[cat_cols])\n",
    "xtest_cat = pd.get_dummies(xtest[cat_cols])\n",
    "print(xtrain_cat.shape)\n",
    "print(xtest_cat.shape)\n",
    "print(list(xtrain_cat))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Joining numeric and categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(16512, 16)\n",
      "(4128, 16)\n"
     ]
    }
   ],
   "source": [
    "xtrain_trans = np.concatenate((xtrain_num, xtrain_cat), axis=1)\n",
    "xtest_trans = np.concatenate((xtest_num, xtest_cat), axis=1)\n",
    "print(xtrain_trans.shape)\n",
    "print(xtest_trans.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Select model using cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_validate\n",
    "\n",
    "# using mean squared error and r2_score as metrics for cross validation training\n",
    "scoring = {'mse': 'neg_mean_squared_error',\n",
    "           'r2': 'r2'\n",
    "           }\n",
    "\n",
    "def print_scores(scores):\n",
    "    \"\"\"\n",
    "    print out scores\n",
    "    :param scores:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    print(\"train rmse: %.2f\" % np.sqrt(-scores[\"train_mse\"]).mean())\n",
    "    print(\"train r2 score: %.2f\" % scores[\"train_r2\"].mean())\n",
    "    print(\"validation rmse: %.2f\" % np.sqrt(-scores[\"test_mse\"]).mean())\n",
    "    print(\"validation score: %.2f\" % scores[\"test_r2\"].mean())\n",
    "    \n",
    "\n",
    "def display_scores(scores):\n",
    "    print(\"Scores:\", scores)\n",
    "    print(\"Mean:\", scores.mean())\n",
    "    print(\"Standard deviation:\", scores.std())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Linear model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Linear regression:\n",
      "\n",
      "train rmse: 68605.81\n",
      "train r2 score: 0.65\n",
      "validation rmse: 69052.46\n",
      "validation score: 0.64\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "lin = LinearRegression()\n",
    "\n",
    "lin_scores = cross_validate(lin, xtrain_trans, ytrain, scoring=scoring,\n",
    "                        cv=10, return_train_score=True)\n",
    "print(\"\\n\\nLinear regression:\\n\")\n",
    "\n",
    "# save model\n",
    "joblib.dump(lin, \"linear_regressor.pkl\")\n",
    "joblib.dump(lin_scores, \"linear_scores.pkl\")\n",
    "\n",
    "print_scores(lin_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Decision Tree:\n",
      "\n",
      "train rmse: 0.00\n",
      "train r2 score: 1.00\n",
      "validation rmse: 71296.64\n",
      "validation score: 0.62\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "dec_tree = DecisionTreeRegressor()\n",
    "dt_scores = cross_validate(dec_tree, xtrain_trans, ytrain, scoring=scoring,\n",
    "                        cv=10, return_train_score=True)\n",
    "# save model\n",
    "joblib.dump(dec_tree, \"dtree_regressor.pkl\")\n",
    "joblib.dump(dt_scores, \"dtree_scores.pkl\")\n",
    "\n",
    "print(\"\\n\\nDecision Tree:\\n\")\n",
    "print_scores(dt_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### RandomForest\n",
    "\n",
    "Random Forests work by training many Decision Trees on random subsets of the features, then averaging out their predictions. Building a model on top of many other models is called Ensemble Learning, and it is often a great way to push ML algorithms even further."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Random Forest:\n",
      "\n",
      "train rmse: 22463.36\n",
      "train r2 score: 0.96\n",
      "validation rmse: 52834.54\n",
      "validation score: 0.79\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "rf = RandomForestRegressor()\n",
    "\n",
    "rf_scores = cross_validate(rf, xtrain_trans, ytrain, scoring=scoring,\n",
    "                        cv=10, return_train_score=True)\n",
    "\n",
    "# save model\n",
    "joblib.dump(rf, \"rf_regressor.pkl\")\n",
    "joblib.dump(rf_scores, \"rf_scores.pkl\")\n",
    "\n",
    "print(\"\\n\\nRandom Forest:\\n\")\n",
    "print_scores(rf_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Random Forest:\n",
      "\n",
      "train rmse: 50731.24\n",
      "train r2 score: 0.81\n",
      "validation rmse: 53109.23\n",
      "validation score: 0.79\n"
     ]
    }
   ],
   "source": [
    "from xgboost import XGBRegressor\n",
    "\n",
    "\n",
    "xb = XGBRegressor()\n",
    "xb_scores = cross_validate(xb, xtrain_trans, ytrain, scoring=scoring,\n",
    "                        cv=10, return_train_score=True)\n",
    "\n",
    "# save model\n",
    "joblib.dump(xb, \"xb_regressor.pkl\")\n",
    "joblib.dump(xb_scores, \"xb_scores.pkl\")\n",
    "\n",
    "print(\"\\n\\nRandom Forest:\\n\")\n",
    "print_scores(xb_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fine-tune model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using GridSearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5, error_score='raise',\n",
       "       estimator=RandomForestRegressor(bootstrap=True, criterion='mse', max_depth=None,\n",
       "           max_features='auto', max_leaf_nodes=None,\n",
       "           min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "           min_samples_leaf=1, min_samples_split=2,\n",
       "           min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=1,\n",
       "           oob_score=False, random_state=None, verbose=0, warm_start=False),\n",
       "       fit_params=None, iid=True, n_jobs=1,\n",
       "       param_grid=[{'n_estimators': [3, 10, 30], 'max_features': [2, 4, 6, 8]}, {'bootstrap': [False], 'n_estimators': [3, 10], 'max_features': [2, 3, 4]}],\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score=True,\n",
       "       scoring='neg_mean_squared_error', verbose=0)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "param_grid = [\n",
    "    {\"n_estimators\":[3, 10, 30], \"max_features\":[2,4,6,8]},\n",
    "    {\"bootstrap\":[False], \"n_estimators\":[3, 10], \"max_features\":[2,3,4]}\n",
    "]\n",
    "\n",
    "forest_reg = RandomForestRegressor()\n",
    "grid_search = GridSearchCV(forest_reg, param_grid, cv=5, scoring=\"neg_mean_squared_error\", return_train_score=True)\n",
    "grid_search.fit(xtrain_trans, ytrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'max_features': 8, 'n_estimators': 30}\n"
     ]
    }
   ],
   "source": [
    "print(grid_search.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestRegressor(bootstrap=True, criterion='mse', max_depth=None,\n",
       "           max_features=8, max_leaf_nodes=None, min_impurity_decrease=0.0,\n",
       "           min_impurity_split=None, min_samples_leaf=1,\n",
       "           min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "           n_estimators=30, n_jobs=1, oob_score=False, random_state=None,\n",
       "           verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_search.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['mean_fit_time', 'std_fit_time', 'mean_score_time', 'std_score_time', 'param_max_features', 'param_n_estimators', 'param_bootstrap', 'params', 'split0_test_score', 'split1_test_score', 'split2_test_score', 'split3_test_score', 'split4_test_score', 'mean_test_score', 'std_test_score', 'rank_test_score', 'split0_train_score', 'split1_train_score', 'split2_train_score', 'split3_train_score', 'split4_train_score', 'mean_train_score', 'std_train_score'])\n"
     ]
    }
   ],
   "source": [
    "cvres = grid_search.cv_results_\n",
    "\n",
    "print(cvres.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "64425.23872443877 {'max_features': 2, 'n_estimators': 3}\n",
      "55257.24295868718 {'max_features': 2, 'n_estimators': 10}\n",
      "52797.815600453396 {'max_features': 2, 'n_estimators': 30}\n",
      "60619.728379472195 {'max_features': 4, 'n_estimators': 3}\n",
      "53154.7169911524 {'max_features': 4, 'n_estimators': 10}\n",
      "50569.60484633807 {'max_features': 4, 'n_estimators': 30}\n",
      "59330.117203289235 {'max_features': 6, 'n_estimators': 3}\n",
      "52071.51052192515 {'max_features': 6, 'n_estimators': 10}\n",
      "50141.734947929916 {'max_features': 6, 'n_estimators': 30}\n",
      "59244.32038081152 {'max_features': 8, 'n_estimators': 3}\n",
      "51818.80762530009 {'max_features': 8, 'n_estimators': 10}\n",
      "50007.79571275452 {'max_features': 8, 'n_estimators': 30}\n",
      "62187.00579667948 {'bootstrap': False, 'max_features': 2, 'n_estimators': 3}\n",
      "54730.672533001416 {'bootstrap': False, 'max_features': 2, 'n_estimators': 10}\n",
      "58379.93836315784 {'bootstrap': False, 'max_features': 3, 'n_estimators': 3}\n",
      "52547.82376020723 {'bootstrap': False, 'max_features': 3, 'n_estimators': 10}\n",
      "58499.63249089273 {'bootstrap': False, 'max_features': 4, 'n_estimators': 3}\n",
      "51910.77991331974 {'bootstrap': False, 'max_features': 4, 'n_estimators': 10}\n"
     ]
    }
   ],
   "source": [
    "for mean_score, params in zip(cvres['mean_test_score'], cvres['params']):\n",
    "    print(np.sqrt(-mean_score), params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using random_search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Analyze the best model and their errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_importances = grid_search.best_estimator_.feature_importances_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[6.74265071e-02 6.27745324e-02 4.45390393e-02 1.47554592e-02\n",
      " 1.49211945e-02 1.47899183e-02 1.40352329e-02 3.90621992e-01\n",
      " 3.58154821e-02 1.09605443e-01 6.51105552e-02 1.97445532e-03\n",
      " 4.55290592e-03 1.55883816e-01 3.11623755e-03 7.72296397e-05]\n"
     ]
    }
   ],
   "source": [
    "print(feature_importances)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let’s display these importance scores next to their corresponding attribute names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "extra_attribs = [\"rooms_per_hhold\", \"pop_per_hhold\", \"bedrooms_per_room\"]\n",
    "cat_one_hot_attribs = list(xtrain_cat)\n",
    "attributes = num_att + extra_attribs + cat_one_hot_attribs "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0.3906219920329171, 'median_income'),\n",
       " (0.15588381564032533, 'ocean_proximity_INLAND'),\n",
       " (0.10960544271748147, 'pop_per_hhold'),\n",
       " (0.06742650713076309, 'longitude'),\n",
       " (0.06511055522255164, 'bedrooms_per_room'),\n",
       " (0.06277453240885793, 'latitude'),\n",
       " (0.044539039349090453, 'housing_median_age'),\n",
       " (0.03581548212058292, 'rooms_per_hhold'),\n",
       " (0.014921194485299282, 'total_bedrooms'),\n",
       " (0.014789918343657676, 'population'),\n",
       " (0.014755459196293902, 'total_rooms'),\n",
       " (0.014035232925918881, 'households'),\n",
       " (0.004552905920272784, 'ocean_proximity_<1H OCEAN'),\n",
       " (0.003116237549921739, 'ocean_proximity_NEAR OCEAN'),\n",
       " (0.0019744553163461155, 'ocean_proximity_NEAR BAY'),\n",
       " (7.722963971963629e-05, 'ocean_proximity_ISLAND')]"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(zip(feature_importances, attributes), reverse=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make prediction on the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "47839.40793014073\n",
      "0.8243806205835597\n"
     ]
    }
   ],
   "source": [
    "final_model = grid_search.best_estimator_\n",
    "\n",
    "ypred_test = final_model.predict(xtest_trans)\n",
    "\n",
    "print(np.sqrt(mean_squared_error(ytest, ypred_test)))\n",
    "print(r2_score(ytest, ypred_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
